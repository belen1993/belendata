{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importación de librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Piconnect in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.12.1)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from Piconnect) (2.2.3)\n",
            "Requirement already satisfied: wrapt in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from Piconnect) (1.16.0)\n",
            "Requirement already satisfied: pythonnet in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from Piconnect) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->Piconnect) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->Piconnect) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->Piconnect) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->Piconnect) (2024.1)\n",
            "Requirement already satisfied: clr_loader<0.3.0,>=0.2.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pythonnet->Piconnect) (0.2.7.post0)\n",
            "Requirement already satisfied: cffi>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from clr_loader<0.3.0,>=0.2.7->pythonnet->Piconnect) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->Piconnect) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.17->clr_loader<0.3.0,>=0.2.7->pythonnet->Piconnect) (2.22)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install Piconnect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Failed to create a default .NET runtime, which would\n                    have been \"mono\" on this system. Either install a\n                    compatible runtime or configure it explicitly via\n                    `set_runtime` or the `PYTHONNET_*` environment variables\n                    (see set_runtime_from_env).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pythonnet/__init__.py:75\u001b[0m, in \u001b[0;36m_create_runtime_from_spec\u001b[0;34m(spec, params)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m spec \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmono\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclr_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m spec \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoreclr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/clr_loader/__init__.py:73\u001b[0m, in \u001b[0;36mget_mono\u001b[0;34m(config_file, global_config_file, libmono, sgen, debug, jit_options, assembly_dir, config_dir, set_signal_chaining)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m libmono \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     libmono \u001b[38;5;241m=\u001b[39m \u001b[43mfind_libmono\u001b[49m\u001b[43m(\u001b[49m\u001b[43msgen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massembly_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massembly_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m impl \u001b[38;5;241m=\u001b[39m Mono(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# domain=domain,\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     set_signal_chaining\u001b[38;5;241m=\u001b[39mset_signal_chaining,\n\u001b[1;32m     85\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/clr_loader/util/find.py:149\u001b[0m, in \u001b[0;36mfind_libmono\u001b[0;34m(assembly_dir, sgen)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find libmono\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Path(path)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not find libmono",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Librerias PI\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIconnect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIData, PIServer, PIConfig\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Librerias de Python\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/PIconnect/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"PIconnect - Connector to the OSISoft PI and PI-AF databases.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# pragma pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAFSDK\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AF, AF_SDK_VERSION\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIConfig\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIconnect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPI\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PIServer\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/PIconnect/AFSDK.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m     _af, _System, _AF_SDK_version \u001b[38;5;241m=\u001b[39m __fallback()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mclr\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Get the installation directory from the environment variable or fall back\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# to the Windows default installation path\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     installation_directories \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     43\u001b[0m         os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPIHOME\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProgram Files\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPIPC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProgram Files (x86)\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPIPC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m     ]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/clr.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mLegacy Python.NET loader for backwards compatibility\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpythonnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m----> 6\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pythonnet/__init__.py:133\u001b[0m, in \u001b[0;36mload\u001b[0;34m(runtime, **params)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _RUNTIME \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m runtime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m         \u001b[43mset_runtime_from_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m         set_runtime(runtime, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pythonnet/__init__.py:114\u001b[0m, in \u001b[0;36mset_runtime_from_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environ\n\u001b[1;32m    113\u001b[0m spec \u001b[38;5;241m=\u001b[39m environ\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONNET_RUNTIME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m runtime \u001b[38;5;241m=\u001b[39m \u001b[43m_create_runtime_from_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m set_runtime(runtime)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pythonnet/__init__.py:82\u001b[0m, in \u001b[0;36m_create_runtime_from_spec\u001b[0;34m(spec, params)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m was_default:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFailed to create a default .NET runtime, which would\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124m                have been \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m on this system. Either install a\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124m                compatible runtime or configure it explicitly via\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124m                `set_runtime` or the `PYTHONNET_*` environment variables\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124m                (see set_runtime_from_env).\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFailed to create a .NET runtime (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) using the\u001b[39m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124m            parameters \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     93\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to create a default .NET runtime, which would\n                    have been \"mono\" on this system. Either install a\n                    compatible runtime or configure it explicitly via\n                    `set_runtime` or the `PYTHONNET_*` environment variables\n                    (see set_runtime_from_env)."
          ]
        }
      ],
      "source": [
        "#Librerias PI\n",
        "from PIconnect import PIData, PIServer, PIConfig\n",
        "\n",
        "# Librerias de Python\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones para DESCARGA DATOS PI OSISOFT AVEVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuración del servidor PI\n",
        "PIServer.DEFAULT_SERVER = 'uwgepi'  # Cambia por el nombre de tu servidor\n",
        "usuario = 'UF183530'  # Cambia por tu usuario\n",
        "contraseña = 'UF183530'  # Cambia por tu contraseña\n",
        "# Configurar la zona horaria predeterminada de PIconnect\n",
        "PIConfig.DEFAULT_TIMEZONE = 'Europe/Madrid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fecha inicial (yyyy, mm, dd, hh, mm, ss)\n",
        "fecha_inicio = datetime(2020, 1, 1, 0, 0, 0)  # Cambia por la fecha de inicio deseada\n",
        "# Fecha fin: se puede poner un delta de tiempo \n",
        "# fecha_fin =fecha_inicio + timedelta(days=20)\n",
        "fecha_fin = datetime(2024, 12, 31, 0, 0, 0)  # Cambia por el rango de tiempo deseado\n",
        "intervalo = \"10s\" # Formato PI\n",
        "# Definicion de tags a descargar\n",
        "tags = {\n",
        "    \"PDB:G1_BTJ1_1\",\n",
        "    \"PDB:G1_BTJ1_2\",\n",
        "    \"PDB:G1_LTB1D\",\n",
        "    \"PDB:G1_BTJ2_1\",\n",
        "    \"PDB:G1_BTJ2_2\",\n",
        "    \"PDB:G1_LTB2D\",\n",
        "    \"PDB:G1_TNH_V\",\n",
        "    \"PDB:G1_DWATT\",\n",
        "    \"PDB:S1_tt_1Sb\",\n",
        "    \"PDB:S1_rotor_exp1\",\n",
        "    \"PDB:S1_sed1\",\n",
        "    \"PDB:S1_axial1\",\n",
        "    \"PDB:S1_axial2\",\n",
        "    \"PDB:G1_BTTA1_6\",\n",
        "    \"PDB:G1_BTTI1_6\",\n",
        "    \"PDB:G1_ctim\",\n",
        "    \"PDB:G1_cpd1a\",\n",
        "    \"PDB:G1_afq\",\n",
        "    \"PDB:G1_afpbd\",\n",
        "    \"PDB:S1_eccentric1\"\n",
        "}\n",
        "# Filtro para descarga. Dejar vacio sino se quiere filtro\n",
        "# Los tags deben ir entre ''\n",
        "#filtro = \"'SAB:G1.TNH_V'>2995\" \n",
        "filtro = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descarga datos de PI en carpeta \"data\" \n",
        "# Crea un fichero por cada mes del año\n",
        "\n",
        "# Conexión al servidor\n",
        "with PIServer() as server:\n",
        "    # Iterar de mes en mes entre las fechas especificadas\n",
        "    fecha_actual = fecha_inicio\n",
        "    while fecha_actual < fecha_fin:\n",
        "        # Calcular el final del mes actual\n",
        "        mes_siguiente = (fecha_actual.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
        "        fecha_siguiente = min(mes_siguiente, fecha_fin)\n",
        "        \n",
        "        # Crear un DataFrame vacío para el mes actual\n",
        "        df_mes = pd.DataFrame()\n",
        "        \n",
        "        for tag in tags:\n",
        "            print(f\"Descargando datos para {tag} desde {fecha_actual} hasta {fecha_siguiente}...\")\n",
        "            # Buscar la etiqueta en el servidor\n",
        "            punto = server.search(tag)\n",
        "            if not punto:\n",
        "                print(f\"Advertencia: No se encontró la etiqueta {tag}\")\n",
        "                continue\n",
        "            \n",
        "            punto = punto[0]  # Toma el primer resultado de la búsqueda\n",
        "            \n",
        "            # Recuperar datos interpolados para el rango mensual\n",
        "            valores = punto.interpolated_values(fecha_actual, fecha_siguiente, intervalo, filtro)\n",
        "            \n",
        "            # Convertir a un DataFrame temporal para normalizar los datos\n",
        "            df_temp = pd.DataFrame(valores.items(), columns=[\"Timestamp\", tag])\n",
        "            df_temp.set_index(\"Timestamp\", inplace=True)\n",
        "            \n",
        "            # Unir los datos del tag actual al DataFrame del mes\n",
        "            if df_mes.empty:\n",
        "                df_mes = df_temp\n",
        "            else:\n",
        "                df_mes = df_mes.join(df_temp, how='outer')\n",
        "        \n",
        "        # Crear carpeta para el año si no existe\n",
        "        year_folder = os.path.join(\"datos\", str(fecha_actual.year))\n",
        "        os.makedirs(year_folder, exist_ok=True)\n",
        "        \n",
        "        # Guardar el DataFrame del mes en un archivo CSV\n",
        "        csv_filename = os.path.join(year_folder, f\"{fecha_actual.year}_{fecha_actual.month:02d}.csv\")\n",
        "        df_mes.to_csv(csv_filename, sep=';', decimal=',')\n",
        "        \n",
        "        # Avanzar al siguiente mes\n",
        "        fecha_actual = fecha_siguiente\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Crear dataframe de todos los CSV descargados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para crear dataframe\n",
        "def leer_csv_en_carpetas(ruta_base):\n",
        "    # Crear una lista para almacenar los DataFrames\n",
        "    dataframes = []\n",
        "    \n",
        "    # Recorrer carpetas y subcarpetas con os.walk\n",
        "    for carpeta, subcarpetas, archivos in os.walk(ruta_base):\n",
        "        #print(f\"Explorando carpeta: {carpeta}\")  # Depuración\n",
        "        for archivo in archivos:\n",
        "            if archivo.endswith('.csv'):  # Filtrar solo archivos CSV\n",
        "                ruta_completa = os.path.join(carpeta, archivo)\n",
        "                #print(f\"Encontrado archivo: {ruta_completa}\")  # Depuración\n",
        "                try:\n",
        "                    # Leer el archivo CSV\n",
        "                    df = pd.read_csv(ruta_completa, sep=';', decimal=',')\n",
        "                    # Convierto la columna de fechas que viene como texto a formato datatime de pandas\n",
        "                    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce', utc=True)\n",
        "                    dataframes.append(df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error al leer el archivo {ruta_completa}: {e}\")\n",
        "    \n",
        "    # Concatenar todos los DataFrames si hay datos\n",
        "    if dataframes:       \n",
        "        df_combinado = pd.concat(dataframes, ignore_index=True)\n",
        "        # Información básica\n",
        "        print (\"Tipo datos\")\n",
        "        print (df_combinado.info())\n",
        "        contar_valores_distintos = df_combinado.nunique() # Muestra cantidad valores unicos de cada señal\n",
        "        print (\"Valores distintos por columnas:\")\n",
        "        print (contar_valores_distintos)\n",
        "        return df_combinado\n",
        "    else:\n",
        "        print(\"No se encontraron archivos CSV en las subcarpetas.\")\n",
        "        return pd.DataFrame()  # Devuelve un DataFrame vacío si no hay datos\n",
        "    \n",
        "# Funcion para convertir tipo datos\n",
        "def convertir_columnas_a_numerico(df, columnas, tipo_dato = float):\n",
        "    \"\"\"\n",
        "    Convierte las columnas especificadas a datos numéricos, eliminando las filas\n",
        "    con valores no numéricos en esas columnas.\n",
        "\n",
        "    Parámetros:\n",
        "        df (pd.DataFrame): El dataframe a procesar.\n",
        "        columnas (list): Lista de nombres de columnas a convertir.\n",
        "\n",
        "    Retorna:\n",
        "        pd.DataFrame: El dataframe con las columnas convertidas y filas no numéricas eliminadas.\n",
        "    \"\"\"\n",
        "    # Aplicar la conversión a numérico con manejo de errores en las columnas especificadas\n",
        "    for columna in columnas:\n",
        "        df[columna] = pd.to_numeric(df[columna], errors='coerce')\n",
        "\n",
        "    # Eliminar filas con NaN en las columnas seleccionadas\n",
        "    df = df.dropna(subset=columnas)\n",
        "\n",
        "    # Convertir los datos al tipo especificado\n",
        "    df.loc[:, columnas] = df[columnas].astype(tipo_dato)\n",
        "\n",
        "    return df\n",
        "\n",
        "def EDA (df):\n",
        "    # Análsis exploratio de datos\n",
        "    sns.histplot(df)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ruta de la carpeta base donde estan los ficheros CSV(cambiar por la tuya)\n",
        "ruta_base = 'datos_PACS'\n",
        "\n",
        "# Llamar a la función para crear el dataframe completo\n",
        "df_leido = leer_csv_en_carpetas(ruta_base)\n",
        "\n",
        "# Convierto a tipo numerico las columnas que necesito\n",
        "# En los datos vendrán valores tipo texto \"BAD\", \"Error\", ... que provienen de PI\n",
        "columnas_a_convertir = ['SAB:CBOP.A81.PAC11.AP001XH01']\n",
        "tipo_conversion = int\n",
        "df_leido = convertir_columnas_a_numerico (df_leido, columnas_a_convertir, tipo_conversion)\n",
        "columnas_a_convertir = ['SAB:CBOP.A81.PAC13.AP001XH01']\n",
        "tipo_conversion = int\n",
        "df_leido = convertir_columnas_a_numerico (df_leido, columnas_a_convertir, tipo_conversion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis exploratorio de datos\n",
        "EDA (df_leido)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contador horas y números arranque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "# dataframe: dataframe a mirar\n",
        "# tag: nombre señal a mirar\n",
        "# estado_marcha: se indica si marcha es un 1 o un 0\n",
        "# intervalo: frecuencia de muestreo\n",
        "\n",
        "def contar_horas_arranques(dataframe, tag, estado_marcha, intervalo):\n",
        "    # Asegurarse de que la columna existe\n",
        "    if tag not in dataframe.columns:\n",
        "        raise ValueError(\"El DataFrame no contiene la señal: \" + tag)\n",
        "    \n",
        "    # Calcular la suma total\n",
        "    suma_total = dataframe[tag].sum()\n",
        "    \n",
        "    # Contar cambios de señal de 0 a 1\n",
        "    # Alternative longer form\n",
        "    if estado_marcha == 1:\n",
        "        estado_marcha = 1\n",
        "        estado_paro = 0 \n",
        "    else:   \n",
        "        estado_marcha = 0\n",
        "        estado_paro = 1\n",
        "\n",
        "    cambios_estado = ((dataframe[tag] == estado_marcha) & (dataframe[tag].shift(1) == estado_paro)).sum()\n",
        "    \n",
        "    segundos_totales = suma_total * int(intervalo)\n",
        "    horas = segundos_totales // 3600\n",
        "    minutos = (segundos_totales % 3600) // 60\n",
        "    return {\n",
        "        'horas_arrancado': horas,\n",
        "        'minutos_arrancado': minutos,\n",
        "        'arranques': cambios_estado\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ejemplo de uso\n",
        "# Primero hago una copia\n",
        "df_tratado = df_leido.copy()\n",
        "\n",
        "tag = \"SAB:CBOP.A81.PAC11.AP001XH01\"\n",
        "estado_marcha = 1\n",
        "intervalo_muestreo_datos = 60 # Ponerlo en segundos\n",
        "resultado = contar_horas_arranques(df_leido, tag, estado_marcha, intervalo_muestreo_datos)\n",
        "\n",
        "print(\"Número arranques:\", resultado['arranques'])\n",
        "# El tiempo arrancado lo \n",
        "print(f\"Tiempo arrancado de {tag}: {resultado['horas_arrancado']} horas y {resultado['minutos_arrancado']} minutos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Máximos y mínimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importamos las bibliotecas necesarias.\n",
        "import pandas as pd  # Para manejar y procesar datos en formato de tablas (DataFrames).\n",
        "from tqdm.notebook import tqdm  # Para mostrar barras de progreso en bucles.\n",
        "import PIconnect as PI  # Para conectarse a un servidor PI y recuperar datos históricos.\n",
        "from PIconnect.PIConsts import SummaryType  # Constantes para definir tipos de resúmenes en PI.\n",
        "from datetime import datetime, timedelta  # Para trabajar con fechas y tiempos.\n",
        "\n",
        "# Función para leer un archivo Excel y convertirlo en un DataFrame de pandas.\n",
        "def leer_excel_a_dataframe(ruta_excel):\n",
        "    try:\n",
        "        # Intentamos leer el archivo Excel y devolverlo como un DataFrame.\n",
        "        df = pd.read_excel(ruta_excel)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        # Si ocurre un error al leer el archivo, se imprime el error y se devuelve un DataFrame vacío.\n",
        "        print(f\"Error al leer el archivo {ruta_excel}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Nombre del archivo Excel que contiene los nombres de las señales (tags).\n",
        "fichero = \"señales_PI_SABON.xlsx\"\n",
        "print(\"Leyendo fichero TAGS\")\n",
        "# Llamamos a la función para leer el archivo Excel.\n",
        "tags_SABON = leer_excel_a_dataframe(fichero)\n",
        "\n",
        "# Definimos un número máximo de tags para procesar (útil para pruebas o evitar procesar demasiados datos).\n",
        "cantidad_tags_a_leer = 10  \n",
        "\n",
        "# Especificamos el rango de fechas para obtener los datos del servidor PI.\n",
        "fecha_inicio = datetime(2024, 1, 1, 0, 0, 0)  # Fecha y hora de inicio.\n",
        "fecha_fin = datetime(2025, 2, 1, 0, 0, 0)  # Fecha y hora de fin.\n",
        "\n",
        "# Conexión al servidor PI.\n",
        "with PI.PIServer() as server:\n",
        "    resultados = []  # Lista para almacenar los resultados de cada tag.\n",
        "    \n",
        "    # Iteramos sobre las primeras filas del DataFrame `tags_SABON` usando tqdm para mostrar el progreso.\n",
        "    for index, row in tqdm(tags_SABON.head(cantidad_tags_a_leer).iterrows(), \n",
        "                           total=cantidad_tags_a_leer, desc=\"Procesando tags\", leave=True):\n",
        "        # Extraemos el nombre del tag y su tipo de dato.\n",
        "        tag_name = row['Name']\n",
        "        tipo = row['pointtype']\n",
        "        print(f\"Procesando: {tag_name} - {tipo}\")\n",
        "        \n",
        "        # Filtramos los tipos de datos que son numéricos y compatibles con operaciones estadísticas.\n",
        "        if tipo in ['Float64', 'Int16', 'Int32', 'Float32']:\n",
        "            try:\n",
        "                # Buscamos el tag en el servidor PI.\n",
        "                points = server.search(tag_name)[0]\n",
        "                fecha_actual = fecha_inicio  # Inicializamos la fecha actual para el rango a procesar.\n",
        "                \n",
        "                # Calculamos el número total de meses en el rango de tiempo especificado.\n",
        "                total_meses = ((fecha_fin.year - fecha_inicio.year) * 12 + fecha_fin.month - fecha_inicio.month) + 1\n",
        "                \n",
        "                # Barra de progreso para mostrar el avance mensual por tag.\n",
        "                with tqdm(total=total_meses-1, desc=f\"Procesando {tag_name}\", leave=False) as pbar:\n",
        "                    while fecha_actual < fecha_fin:\n",
        "                        # Calculamos el inicio del siguiente mes para dividir los datos por períodos mensuales.\n",
        "                        mes_siguiente = (fecha_actual.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
        "                        fecha_siguiente = min(mes_siguiente, fecha_fin)  # Nos aseguramos de no exceder la fecha final.\n",
        "                        \n",
        "                        # Obtenemos los valores máximos y mínimos del tag en el período actual.\n",
        "                        data = points.summary(fecha_actual, fecha_siguiente, SummaryType.MAXIMUM | SummaryType.MINIMUM)\n",
        "                        data['Tag'] = tag_name  # Añadimos una columna con el nombre del tag.\n",
        "                        resultados.append(data)  # Agregamos los datos al resultado acumulado.\n",
        "                        \n",
        "                        fecha_actual = fecha_siguiente  # Actualizamos la fecha para el siguiente período.\n",
        "                        pbar.update(1)  # Actualizamos la barra de progreso.\n",
        "            except Exception as e:\n",
        "                # Si ocurre un error al procesar un tag, lo registramos.\n",
        "                print(f\"Error al obtener datos para {tag_name}: {e}\")\n",
        "\n",
        "    # Una vez procesados todos los tags, concatenamos los resultados en un único DataFrame.\n",
        "    df_resultados = pd.concat(resultados)\n",
        "\n",
        "    # Agrupamos los resultados por cada tag para calcular los valores mínimos y máximos globales,\n",
        "    # así como las marcas de tiempo correspondientes.\n",
        "    df_resultados_unificado = df_resultados.groupby('Tag').agg(\n",
        "        MINIMUM=('MINIMUM', 'min'),  # Valor mínimo global.\n",
        "        MAXIMUM=('MAXIMUM', 'max'),  # Valor máximo global.\n",
        "        TIMESTAMP_MIN=('MINIMUM', 'idxmin'),  # Marca de tiempo del valor mínimo.\n",
        "        TIMESTAMP_MAX=('MAXIMUM', 'idxmax')  # Marca de tiempo del valor máximo.\n",
        "    ).reset_index()\n",
        "\n",
        "    print(\"Finalizado\")\n",
        "    # Exportamos los resultados a un archivo CSV para su análisis posterior.\n",
        "    df_resultados_unificado.to_csv(\"tags_minimos_maximos.csv\", sep=\";\", decimal=\",\", encoding=\"utf-8-sig\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "es"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
